{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py')\n",
    "plaintext_rdd = sc.textFile('datos/caba_para_mapa.csv')\n",
    "dataframe = pycsv.csvToDataFrame(sqlCtx, plaintext_rdd)\n",
    "dataframe_rdd = dataframe.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:97% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:97% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE CABA PORQUE LSH TIRA ERROR CUANDO ENCUENTRA NANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_on', 'timestamp'),\n",
       " ('property_type', 'string'),\n",
       " ('place_name', 'string'),\n",
       " ('state_name', 'string'),\n",
       " ('lat-lon', 'string'),\n",
       " ('lat', 'double'),\n",
       " ('lon', 'double'),\n",
       " ('price', 'int'),\n",
       " ('currency', 'string'),\n",
       " ('price_aprox_local_currency', 'double'),\n",
       " ('price_aprox_usd', 'int'),\n",
       " ('surface_total_in_m2', 'int'),\n",
       " ('surface_covered_in_m2', 'int'),\n",
       " ('price_usd_per_m2', 'int'),\n",
       " ('price_per_m2', 'double'),\n",
       " ('floor', 'int'),\n",
       " ('rooms', 'int'),\n",
       " ('expenses', 'int'),\n",
       " ('properati_url', 'string'),\n",
       " ('description', 'string'),\n",
       " ('title', 'string'),\n",
       " ('dist_a_subte', 'double'),\n",
       " ('dist_a_tren', 'double'),\n",
       " ('dist_a_univ', 'double'),\n",
       " ('dist_a_villa', 'double'),\n",
       " ('dist_a_zona_anegada', 'double')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "#preparo las columnas para pasarle lsh\n",
    "assembler = VectorAssembler(inputCols=[\"surface_total_in_m2\",\"dist_a_subte\"], outputCol=\"vector2\")\n",
    "dataframe = assembler.transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|    vector2|\n",
      "+-----------+\n",
      "| [29.0,2.7]|\n",
      "| [47.0,7.3]|\n",
      "| [47.0,7.3]|\n",
      "| [86.0,5.8]|\n",
      "| [65.0,9.3]|\n",
      "| [35.0,1.9]|\n",
      "|[153.0,4.5]|\n",
      "| [40.0,3.7]|\n",
      "| [40.0,3.7]|\n",
      "| [40.0,3.7]|\n",
      "|[470.0,8.7]|\n",
      "| [70.0,7.0]|\n",
      "| [47.0,4.3]|\n",
      "| [48.0,4.3]|\n",
      "| [33.0,4.3]|\n",
      "| [28.0,4.3]|\n",
      "| [26.0,4.3]|\n",
      "| [48.0,4.3]|\n",
      "| [50.0,4.3]|\n",
      "|[270.0,9.4]|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select(\"vector2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Approximately searching dfA for 20 nearest neighbors of', DenseVector([51.3, 0.62]))\n",
      "+------------+----------+--------------+-------------------+-------+\n",
      "|  place_name|   vector2|        bucket|          distancia|  price|\n",
      "+------------+----------+--------------+-------------------+-------+\n",
      "|Barrio Norte|[51.0,0.7]|[[0.0], [0.0]]| 0.3104834939251977| 170000|\n",
      "|   Balvanera|[51.0,0.7]|[[0.0], [0.0]]| 0.3104834939251977| 130000|\n",
      "|    Recoleta|[51.0,0.7]|[[0.0], [0.0]]| 0.3104834939251977| 140000|\n",
      "|    Recoleta|[51.0,0.5]|[[0.0], [0.0]]| 0.3231098884280676| 204585|\n",
      "|Barrio Norte|[51.0,0.5]|[[0.0], [0.0]]| 0.3231098884280676| 152000|\n",
      "|    Recoleta|[51.0,0.5]|[[0.0], [0.0]]| 0.3231098884280676| 178187|\n",
      "|    Recoleta|[51.0,0.5]|[[0.0], [0.0]]| 0.3231098884280676| 181486|\n",
      "|    Recoleta|[51.0,0.5]|[[0.0], [0.0]]| 0.3231098884280676| 184786|\n",
      "|    Recoleta|[51.0,0.5]|[[0.0], [0.0]]| 0.3231098884280676| 188746|\n",
      "|    Recoleta|[51.0,0.5]|[[0.0], [0.0]]| 0.3231098884280676| 192705|\n",
      "|    Recoleta|[51.0,0.5]|[[0.0], [0.0]]| 0.3231098884280676| 201285|\n",
      "|    Recoleta|[51.0,0.8]|[[0.0], [0.0]]|0.34985711369071565|2218764|\n",
      "|    Recoleta|[51.0,0.8]|[[0.0], [0.0]]|0.34985711369071565|2313180|\n",
      "|Barrio Norte|[51.0,0.4]|[[0.0], [0.0]]|0.37202150475476314| 145000|\n",
      "|Barrio Norte|[51.0,0.4]|[[0.0], [0.0]]|0.37202150475476314| 135000|\n",
      "|Barrio Norte|[51.0,0.4]|[[0.0], [0.0]]|0.37202150475476314| 170000|\n",
      "|    Recoleta|[51.0,0.4]|[[0.0], [0.0]]|0.37202150475476314| 169000|\n",
      "|     Palermo|[51.0,0.4]|[[0.0], [0.0]]|0.37202150475476314|  93000|\n",
      "|Barrio Norte|[51.0,0.9]|[[0.0], [0.0]]|0.41036569057366173| 145000|\n",
      "|Barrio Norte|[51.0,0.9]|[[0.0], [0.0]]|0.41036569057366173| 165000|\n",
      "+------------+----------+--------------+-------------------+-------+\n",
      "\n",
      "+-------+----------+-------------------+-----------------+\n",
      "|summary|place_name|          distancia|            price|\n",
      "+-------+----------+-------------------+-----------------+\n",
      "|  count|        20|                 20|               20|\n",
      "|   mean|      null|0.34484413607513537|         373886.2|\n",
      "| stddev|      null|0.03215950075862677|647829.4192731195|\n",
      "|    min| Balvanera| 0.3104834939251977|            93000|\n",
      "|    max|  Recoleta|0.41036569057366173|          2313180|\n",
      "+-------+----------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "key = Vectors.dense([51.3,0.62])\n",
    "\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"vector2\", outputCol=\"bucket\", bucketLength=500.0,\n",
    "                                  numHashTables=2)#numHashTables: cantidad de ORs, todavia no esta implementado las ANDs\n",
    "model = brp.fit(dataframe)\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "# neighbor search.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "print(\"Approximately searching dfA for 20 nearest neighbors of\",key)\n",
    "resultado=model.approxNearestNeighbors(dataframe, key, 20,'distancia').select('place_name','vector2','bucket','distancia','price')\n",
    "resultado.show()\n",
    "resultado.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de como pasar un feature continuo a discreto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|surface_total_in_m2|result|\n",
      "+-------------------+------+\n",
      "|                 29|   5.0|\n",
      "|                 47|  22.0|\n",
      "|                 47|  22.0|\n",
      "|                 86|  52.0|\n",
      "|                 65|  38.0|\n",
      "|                 35|  10.0|\n",
      "|                153|  70.0|\n",
      "|                 40|  15.0|\n",
      "|                 40|  15.0|\n",
      "|                 40|  15.0|\n",
      "|                470|  84.0|\n",
      "|                 70|  42.0|\n",
      "|                 47|  22.0|\n",
      "|                 48|  23.0|\n",
      "|                 33|   8.0|\n",
      "|                 28|   5.0|\n",
      "|                 26|   4.0|\n",
      "|                 48|  23.0|\n",
      "|                 50|  25.0|\n",
      "|                270|  80.0|\n",
      "+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "sup= dataframe.select('surface_total_in_m2')\n",
    "discretizer = QuantileDiscretizer(numBuckets=100, inputCol=\"surface_total_in_m2\", outputCol=\"result\")\n",
    "result = discretizer.fit(sup).transform(sup)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los algoritmos de clasificacion piden labeled points como entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<result['result']>\n",
      "Column<surface_total_in_m2['surface_total_in_m2']>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-9b0ddb229cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msuperficie\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurface_total_in_m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msuperficie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuperficie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/Documentos/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, label, features)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/Documentos/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\u001b[0m in \u001b[0;36m_convert_to_vector\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDenseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_have_scipy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Expected column vector\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/Documentos/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ar)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "bucket= result.select('result')\n",
    "bucket=bucket.\n",
    "print bucket\n",
    "superficie= result[0].surface_total_in_m2\n",
    "print superficie\n",
    "pos = LabeledPoint(label,[superficie])\n",
    "print pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento con mas de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sup= dataframe.select('surface_total_in_m2','price')\n",
    "discretizer = QuantileDiscretizer(numBuckets=100, inputCol=\"surface_total_in_m2\", outputCol=\"resultado\")\n",
    "discretizado = discretizer.fit(sup).transform(sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[surface_total_in_m2: int, price: int, resultado: double]\n",
      "(22.0,[1.0,0.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "discretizado.select('surface_total_in_m2')\n",
    "pos = LabeledPoint(label, [1.0, 0.0, 3.0])\n",
    "print pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostedTrees no es multilabel <span style=\"color:red\"> Solo predice 0 ó 1 </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_on=datetime.datetime(2015, 7, 1, 0, 0), property_type=u'apartment', place_name=u'Palermo', state_name=u'Capital Federal', lat-lon=u'(-34.574594426799997, -58.419254851700003)', lat=-34.5745944268, lon=-58.4192548517, price=71000, currency=u'USD', price_aprox_local_currency=1252759.5, price_aprox_usd=71000, surface_total_in_m2=29, surface_covered_in_m2=29, price_usd_per_m2=2448, price_per_m2=43198.0, floor=None, rooms=1, expenses=1500, properati_url=u'http://www.properati.com.ar/rlpn_venta_departamentos_palermo', description=u'VENTA DEPARTAMENTO DE 1 AMBIENTE CON PATIO EN PALERMO NUEVOentre Kennedy y Oro.Excelente ubicaci\\xc3\\xb3n frente a la Embajada de Estados Unidos, a metros de los Bosques de Palermo y una cuadra de Av. del LibertadorMuy buen estado general. Pisos de parquet. Amplio ambiente con cocina integrada y ba\\xc3\\xb1o completo. Peque\\xc3\\xb1o patio.IMPORTANTE: ESTOS PRECIOS ESTAN SUJETOS A APROBACION DEL PROPIETARIO', title=u'Departamento en Palermo', dist_a_subte=2.7, dist_a_tren=4.2, dist_a_univ=4.1, dist_a_villa=8.6, dist_a_zona_anegada=8.0),\n",
       " Row(created_on=datetime.datetime(2015, 7, 1, 0, 0), property_type=u'apartment', place_name=u'Villa del Parque', state_name=u'Capital Federal', lat-lon=u'(-34.610611015700002, -58.479590288599994)', lat=-34.6106110157, lon=-58.4795902886, price=96000, currency=u'USD', price_aprox_local_currency=1693872.0, price_aprox_usd=96000, surface_total_in_m2=47, surface_covered_in_m2=42, price_usd_per_m2=2042, price_per_m2=36039.0, floor=None, rooms=2, expenses=1, properati_url=u'http://www.properati.com.ar/rls7_venta_departamentos_villa-del-parque', description=u'ESPECTACULAR DOS AMBIENTES DE 47M2 TOTALES EN EDIFICIO A ESTRENAR. 6TO. PISO AL FRENTE. AMPLIO LIVING COMEDOR CON PISOS DE PORCELANATO Y COCINA INTEGRADA COMPLETA A BALCON A AIRE Y LUZ ABIERTO. BA\\xc3\\x91O COMPLETO. DORMITORIO TAMBIEN CON PISOS DE PORCELANATO A BALCON AL FRENTE CON VISTA ABIERTA. TODO LUZ Y SOL. EDIFICIO CON SUM Y SOLARIUM. APTO PROFESIONAL. INMEJORABLE UBICACION A METROS DE AV. NAZCA Y A POCAS CUADRAS DE AV. JUAN B. JUSTO CON METROBUS.***TASAMOS EN EL DIA // ATENDEMOS TODOS LOS DIAS*** *** // ***Las medidas, superficies y valores de esta ficha son aproximadas y a t\\xc3\\xadtulo informativo \\xc3\\xbanicamente.SI TENES QUE VENDER PARA COMPRAR LEXINTON ES LA INMOBILIARIA POR EXCELENCIA EN COMPRA/ VENTA SIMULTANEA. COMUN\\xc3\\x8dCATE AL   // TASACIONES PROFESIONALESLas medidas, superficies y valores de esta ficha son aproximadas y a t\\xc3\\xadtulo informativo \\xc3\\xbanicamente.', title=u'SAN BLAS 2700 6\\xc2\\xba-  A ESTRENAR', dist_a_subte=7.3, dist_a_tren=8.0, dist_a_univ=8.9, dist_a_villa=8.8, dist_a_zona_anegada=2.8)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file.\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = dataframe_rdd.randomSplit([0.7, 0.3])\n",
    "trainingData.take(2)\n",
    "\n",
    "model = GradientBoostedTrees.trainRegressor(trainingData,\n",
    "                                            categoricalFeaturesInfo={}, numIterations=3)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(testData.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print('Learned regression GBT model:')\n",
    "print(model.toDebugString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "--------------------------\n",
      "[(1.0, 1.0), (0.0, 0.0), (1.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (1.0, 1.0)]\n",
      "--------------------------\n",
      "Test Mean Squared Error = 0.0416666666667\n",
      "Learned regression GBT model:\n",
      "TreeEnsembleModel regressor with 5 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 434 <= 0.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 434 > 0.0)\n",
      "     Predict: 1.0\n",
      "  Tree 1:\n",
      "    Predict: 0.0\n",
      "  Tree 2:\n",
      "    Predict: 0.0\n",
      "  Tree 3:\n",
      "    Predict: 0.0\n",
      "  Tree 4:\n",
      "    Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "numero= 10\n",
    "# Load and parse the data file.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"datos/sample_libsvm_data.txt\")\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GradientBoostedTrees model.\n",
    "#  Notes: (a) Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#         (b) Use more iterations in practice.\n",
    "model = GradientBoostedTrees.trainRegressor(trainingData,\n",
    "                                            categoricalFeaturesInfo={}, numIterations=5)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "print predictions.take(numero)\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "print (\"--------------------------\")\n",
    "print labelsAndPredictions.take(numero)\n",
    "print (\"--------------------------\")\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(testData.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print('Learned regression GBT model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No le di mas bola a GBT cuando me entere que no era multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(created_on=datetime.datetime(2015, 7, 1, 0, 0), property_type=u'apartment', place_name=u'Balvanera', state_name=u'Capital Federal', lat-lon=u'(-34.605761600000001, -58.393765199999997)', lat=-34.6057616, lon=-58.3937652, price=75000, currency=u'USD', price_aprox_local_currency=1323337.5, price_aprox_usd=75000, surface_total_in_m2=49, surface_covered_in_m2=None, price_usd_per_m2=1530, price_per_m2=27006.0, floor=None, rooms=2, expenses=None, properati_url=u'http://www.properati.com.ar/rm0h_venta_departamentos_balvanera_riobamba_300', description=u'-------NO ES DUE\\xc3\\x91O DIRECTO---------------------------------------------------------Se vende depto de 2 amb de 49 m2 totales, al frente con balc\\xc3\\xb3n aterrazado. Cocina separada. Muy luminoso y en muy buen estado. Riobamba al 300 entre Corrientes y Sarmiento. A media cuadra de Av. Corrientes y Est. Callao -Subte l\\xc3\\xadnea B. Y una cuadra de Av. Callao. Riobamba Al 300 Esquina Corrientes, Balvanera.', title=u'Departamentos venta', dist_a_subte=1.5, dist_a_tren=8.4, dist_a_univ=1.4, dist_a_villa=4.9, dist_a_zona_anegada=10.1), Row(created_on=datetime.datetime(2015, 7, 1, 0, 0), property_type=u'apartment', place_name=u'Balvanera', state_name=u'Capital Federal', lat-lon=u'(-34.610005508500002, -58.394939248699998)', lat=-34.6100055085, lon=-58.3949392487, price=53500, currency=u'USD', price_aprox_local_currency=943980.75, price_aprox_usd=53500, surface_total_in_m2=22, surface_covered_in_m2=22, price_usd_per_m2=2431, price_per_m2=42908.0, floor=None, rooms=None, expenses=None, properati_url=u'http://www.properati.com.ar/rm2r_venta_departamentos_balvanera', description=u'CODIGO:  ubicado en: Sarand\\xc3\\xad 100 -  Publicado por: AMBIENTE 23 NEGOCIOS INMOBILIARIOS. El precio es de USD 53500 null. EXCELENTE PROPIEDAD, MUY BUEN ESTADO , cocina completa , alacena y bajo mesada nueva, griferia con mono comando . ambiente principal con balc\\xc3\\xb3n franc\\xc3\\xa9s, piso de madera, aire acondicionado , 2 bibliotecas , 1 placard , T.E  . Publicado a trav\\xc3\\xa9s de Mapaprop', title=u'Sarand\\xc3\\xad 100 1 ambiente - Balvanera', dist_a_subte=1.9, dist_a_tren=8.7, dist_a_univ=1.8, dist_a_villa=4.4, dist_a_zona_anegada=10.1)]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "the data should be RDD of LabeledPoint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-d506d9fbcceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n\u001b[1;32m     13\u001b[0m                                      \u001b[0mnumTrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureSubsetStrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                      impurity='gini', maxDepth=4, maxBins=32)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Evaluate model on test instances and compute test error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/Documentos/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\u001b[0m in \u001b[0;36mtrainClassifier\u001b[0;34m(cls, data, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed)\u001b[0m\n\u001b[1;32m    404\u001b[0m         return cls._train(data, \"classification\", numClasses,\n\u001b[1;32m    405\u001b[0m                           \u001b[0mcategoricalFeaturesInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumTrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureSubsetStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpurity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                           maxDepth, maxBins, seed)\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/Documentos/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(cls, data, algo, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed)\u001b[0m\n\u001b[1;32m    304\u001b[0m                featureSubsetStrategy, impurity, maxDepth, maxBins, seed):\n\u001b[1;32m    305\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"the data should be RDD of LabeledPoint\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeatureSubsetStrategy\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupportedFeatureSubsetStrategies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unsupported featureSubsetStrategy: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfeatureSubsetStrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: the data should be RDD of LabeledPoint"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = dataframe_rdd.randomSplit([0.7, 0.3])\n",
    "print trainingData.take(2)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "print (\"--------------------------\")\n",
    "print predictions.take(numero)\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "print (\"--------------------------\")\n",
    "print labelsAndPredictions.take(numero)\n",
    "print (\"--------------------------\")\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
