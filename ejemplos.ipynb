{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py')\n",
    "plaintext_rdd = sc.textFile('datos/caba_para_mapa.csv')\n",
    "dataframe = pycsv.csvToDataFrame(sqlCtx, plaintext_rdd)\n",
    "data = dataframe.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:97% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:97% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[created_on: timestamp, property_type: string, place_name: string, state_name: string, lat-lon: string, lat: double, lon: double, price: int, currency: string, price_aprox_local_currency: double, price_aprox_usd: int, surface_total_in_m2: int, surface_covered_in_m2: int, price_usd_per_m2: int, price_per_m2: double, floor: int, rooms: int, expenses: int, properati_url: string, description: string, title: string, dist_a_subte: double, dist_a_tren: double, dist_a_univ: double, dist_a_villa: double, dist_a_zona_anegada: double]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe\n",
    "#dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|bucketsDeSubtes|\n",
      "+---------------+\n",
      "|            1.0|\n",
      "|            3.0|\n",
      "|            3.0|\n",
      "|            2.0|\n",
      "|            4.0|\n",
      "|            0.0|\n",
      "|            2.0|\n",
      "|            1.0|\n",
      "|            1.0|\n",
      "|            1.0|\n",
      "|            4.0|\n",
      "|            3.0|\n",
      "|            2.0|\n",
      "|            2.0|\n",
      "|            2.0|\n",
      "|            2.0|\n",
      "|            2.0|\n",
      "|            2.0|\n",
      "|            2.0|\n",
      "|            4.0|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transforma datos continuos en discreto mediante buckets, los buckets los definis vos\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "splits = []\n",
    "for x in range(0, 10):\n",
    "    splits.append(x*2)\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"dist_a_subte\", outputCol=\"bucketsDeSubtes\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataframe)\n",
    "\n",
    "# print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.select('bucketsDeSubtes').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|bucketsDeSubtes|\n",
      "+---------------+\n",
      "|            3.0|\n",
      "|            8.0|\n",
      "|            8.0|\n",
      "|            6.0|\n",
      "|            9.0|\n",
      "|            1.0|\n",
      "|            5.0|\n",
      "|            4.0|\n",
      "|            4.0|\n",
      "|            4.0|\n",
      "|            9.0|\n",
      "|            7.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            9.0|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transforma datos continuos en discreto mediante buckets, le indicas la cantidad de buckets que queres, supongo que \n",
    "# primero revisa cual es el mayor\n",
    "# no es exacto porque usa aproximaciones (approxQuantile), para eso esta relativeError\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=10, inputCol=\"dist_a_subte\", outputCol=\"bucketsDeSubtes\")\n",
    "\n",
    "result = discretizer.fit(dataframe).transform(dataframe)\n",
    "result.select('bucketsDeSubtes').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxAbsScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|      features|  scaledFeatures|\n",
      "+--------------+----------------+\n",
      "|[1.0,0.1,-8.0]|[0.25,0.01,-1.0]|\n",
      "|[2.0,1.0,-4.0]|  [0.5,0.1,-0.5]|\n",
      "|[4.0,10.0,8.0]|   [1.0,1.0,1.0]|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -8.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, -4.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 8.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1.000000]\n",
      "+--------------+--------------+\n",
      "|      features|scaledFeatures|\n",
      "+--------------+--------------+\n",
      "|[1.0,0.1,-1.0]| [0.0,0.0,0.0]|\n",
      "| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n",
      "|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------------------------------------+------+\n",
      "|sentence                              |words                                     |tokens|\n",
      "+--------------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark                |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes    |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,425-351|[logistic,regression,models,are,425-351]  |1     |\n",
      "+--------------------------------------+------------------------------------------+------+\n",
      "\n",
      "+--------------------------------------+---------------------------------------------+------+\n",
      "|sentence                              |words                                        |tokens|\n",
      "+--------------------------------------+---------------------------------------------+------+\n",
      "|Hi I heard about Spark                |[hi, i, heard, about, spark]                 |5     |\n",
      "|I wish Java could use case classes    |[i, wish, java, could, use, case, classes]   |7     |\n",
      "|Logistic,regression,models,are,425-351|[logistic, regression, models, are, 425, 351]|6     |\n",
      "+--------------------------------------+---------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,425-351\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[0,5,9,17],[0...|\n",
      "|  0.0|(20,[2,7,9,13,15]...|\n",
      "|  1.0|(20,[4,6,13,15,18...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---+-----------+-----------------------+\n",
      "|id |features   |hashes                 |\n",
      "+---+-----------+-----------------------+\n",
      "|0  |[1.0,1.0]  |[[0.0], [0.0], [-1.0]] |\n",
      "|1  |[1.0,-1.0] |[[-1.0], [0.0], [0.0]] |\n",
      "|2  |[-1.0,-1.0]|[[-1.0], [-1.0], [0.0]]|\n",
      "|3  |[-1.0,1.0] |[[0.0], [-1.0], [-1.0]]|\n",
      "+---+-----------+-----------------------+\n",
      "\n",
      "Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\n",
      "+---+---+-----------------+\n",
      "|idA|idB|EuclideanDistance|\n",
      "+---+---+-----------------+\n",
      "|0  |6  |1.0              |\n",
      "|2  |5  |1.0              |\n",
      "|2  |7  |1.0              |\n",
      "|1  |4  |1.0              |\n",
      "|1  |7  |1.0              |\n",
      "|0  |4  |1.0              |\n",
      "|3  |6  |1.0              |\n",
      "|3  |5  |1.0              |\n",
      "+---+---+-----------------+\n",
      "\n",
      "Approximately searching dfA for 2 nearest neighbors of the key:\n",
      "+---+----------+----------------------+-------+\n",
      "|id |features  |hashes                |distCol|\n",
      "+---+----------+----------------------+-------+\n",
      "|0  |[1.0,1.0] |[[0.0], [0.0], [-1.0]]|1.0    |\n",
      "|1  |[1.0,-1.0]|[[-1.0], [0.0], [0.0]]|1.0    |\n",
      "+---+----------+----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataA = [(0, Vectors.dense([1.0, 1.0]),),\n",
    "         (1, Vectors.dense([1.0, -1.0]),),\n",
    "         (2, Vectors.dense([-1.0, -1.0]),),\n",
    "         (3, Vectors.dense([-1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "\n",
    "dataB = [(4, Vectors.dense([1.0, 0.0]),),\n",
    "         (5, Vectors.dense([-1.0, 0.0]),),\n",
    "         (6, Vectors.dense([0.0, 1.0]),),\n",
    "         (7, Vectors.dense([0.0, -1.0]),)]\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n",
    "\n",
    "key = Vectors.dense([1.0, 0.0])\n",
    "\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
    "                                  numHashTables=3)\n",
    "model = brp.fit(dfA)\n",
    "\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show(truncate=False)\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "print(\"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 1.5, distCol=\"EuclideanDistance\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"EuclideanDistance\")).show(truncate=False)\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "# neighbor search.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "print(\"Approximately searching dfA for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(dfA, key, 2).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
